{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NaQFxwE_OrRp",
    "outputId": "44df95c1-bd48-4e66-c738-20a98f2b9c12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Zhltra-mGoZ"
   },
   "source": [
    "# **Importing Libraries :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5mf6OHamUn5-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BUILDING MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x3MyK5tjgz00"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, train_data, test_data=None):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Load the dataset.\"\"\"\n",
    "        print(\"Training data loaded successfully.\")\n",
    "        if self.test_data is not None:\n",
    "            print(\"Test data loaded successfully.\")\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Preprocess the data (split, scale, handle missing values).\"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "\n",
    "        # Handle datetime columns for both train and test data\n",
    "        self._handle_datetime_columns(self.train_data)\n",
    "        if self.test_data is not None:\n",
    "            self._handle_datetime_columns(self.test_data)\n",
    "\n",
    "        # Handle Categorical Variables: One-Hot Encoding for categorical columns in training and test data\n",
    "        self.train_data = self._handle_categorical_columns(self.train_data)\n",
    "        if self.test_data is not None:\n",
    "            self.test_data = self._handle_categorical_columns(self.test_data)\n",
    "\n",
    "        # Align columns between train and test data\n",
    "        if self.test_data is not None:\n",
    "            self.test_data = self._align_columns(self.train_data, self.test_data)\n",
    "\n",
    "        # Split the training dataset into features (X) and target (y)\n",
    "        X = self.train_data.drop(columns='loan_status')  # Assuming 'loan_status' is the target\n",
    "        y = self.train_data['loan_status']\n",
    "\n",
    "        # Train-Test split for training data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Feature Scaling: Only apply to numerical columns for training data\n",
    "        numerical_columns = self.X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "        self.X_train[numerical_columns] = self.scaler.fit_transform(self.X_train[numerical_columns])\n",
    "        self.X_test[numerical_columns] = self.scaler.transform(self.X_test[numerical_columns])\n",
    "\n",
    "        # Apply the same scaling to the test data if it exists\n",
    "        if self.test_data is not None:\n",
    "            X_test_data = self.test_data.drop(columns='loan_status')\n",
    "            self.test_data[numerical_columns] = self.scaler.transform(X_test_data[numerical_columns])\n",
    "\n",
    "        print(\"Preprocessing complete.\")\n",
    "\n",
    "    def _handle_datetime_columns(self, data):\n",
    "        \"\"\"Handle datetime columns (convert to numerical features).\"\"\"\n",
    "        datetime_columns = data.select_dtypes(include=['datetime']).columns\n",
    "        if len(datetime_columns) > 0:\n",
    "            print(f\"Found datetime columns: {datetime_columns}\")\n",
    "            # Convert datetime columns to numerical features (e.g., year, month, day)\n",
    "            for col in datetime_columns:\n",
    "                data[col] = pd.to_datetime(data[col])  # Convert to datetime if not already\n",
    "                data[f'{col}_year'] = data[col].dt.year\n",
    "                data[f'{col}_month'] = data[col].dt.month\n",
    "                data[f'{col}_day'] = data[col].dt.day\n",
    "                data.drop(columns=[col], inplace=True)  # Drop the original datetime column\n",
    "\n",
    "    def _handle_categorical_columns(self, data):\n",
    "        \"\"\"Handle Categorical Variables: One-Hot Encoding for categorical columns.\"\"\"\n",
    "        categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "        print(f\"Found categorical columns: {categorical_columns}\")\n",
    "        data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "        return data\n",
    "\n",
    "    def _align_columns(self, train_data, test_data):\n",
    "        \"\"\"Align the columns of the test data to match the training data's columns.\"\"\"\n",
    "        train_columns = set(train_data.columns)\n",
    "        test_columns = set(test_data.columns)\n",
    "\n",
    "        # Add missing columns in the test data, fill with 0 (or any default value)\n",
    "        missing_in_test = train_columns - test_columns\n",
    "        for col in missing_in_test:\n",
    "            test_data[col] = 0  # Add missing columns to the test data with default values\n",
    "\n",
    "        # Ensure columns in both train and test are in the same order\n",
    "        test_data = test_data[train_data.columns]\n",
    "        return test_data\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        raise NotImplementedError(\"Each model should implement its own training method.\")\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Test the model and generate evaluation summary.\"\"\"\n",
    "        print(\"Testing the model on training data...\")\n",
    "        predictions_train = self.model.predict(self.X_test)\n",
    "        print(\"Training Accuracy Score: \", accuracy_score(self.y_test, predictions_train))\n",
    "        print(\"Training Classification Report: \")\n",
    "        print(classification_report(self.y_test, predictions_train))\n",
    "\n",
    "        if self.test_data is not None:\n",
    "            # Make predictions on the test data\n",
    "            print(\"Testing the model on test data...\")\n",
    "            X_test_data = self.test_data.drop(columns='loan_status')\n",
    "            predictions_test = self.model.predict(X_test_data)\n",
    "            print(\"Test Accuracy Score: \", accuracy_score(self.test_data['loan_status'], predictions_test))\n",
    "            print(\"Test Classification Report: \")\n",
    "            print(classification_report(self.test_data['loan_status'], predictions_test))\n",
    "\n",
    "    def predict(self, new_data):\n",
    "        \"\"\"Predict using the trained model.\"\"\"\n",
    "        predictions = self.model.predict(new_data)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tIVEktefb5eS"
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(Model):\n",
    "    def __init__(self, train_data, test_data=None):\n",
    "        super().__init__(train_data, test_data)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the Logistic Regression model.\"\"\"\n",
    "        print(\"Training Logistic Regression model...\")\n",
    "        self.model = LogisticRegression(max_iter=1000)  # Increase max_iter for convergence\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oDmE_Swzg62p"
   },
   "outputs": [],
   "source": [
    "class RandomForestModel(Model):\n",
    "    def __init__(self, train_data, test_data=None):\n",
    "        super().__init__(train_data, test_data)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the Random Forest model.\"\"\"\n",
    "        print(\"Training Random Forest model...\")\n",
    "        self.model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzk_Q44bhEYI",
    "outputId": "f0174e73-6997-4ff0-a992-dbbad1813dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded successfully.\n",
      "Test data loaded successfully.\n",
      "Preprocessing data...\n",
      "Found datetime columns: Index(['transaction_date'], dtype='object')\n",
      "Found datetime columns: Index(['transaction_date'], dtype='object')\n",
      "Found categorical columns: Index(['sub_grade', 'term', 'home_ownership', 'purpose', 'application_type',\n",
      "       'verification_status'],\n",
      "      dtype='object')\n",
      "Found categorical columns: Index(['sub_grade', 'term', 'home_ownership', 'purpose', 'application_type',\n",
      "       'verification_status'],\n",
      "      dtype='object')\n",
      "Preprocessing complete.\n",
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Testing the model on training data...\n",
      "Training Accuracy Score:  0.7650938832944901\n",
      "Training Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.26      0.37      5917\n",
      "           1       0.78      0.94      0.86     16824\n",
      "\n",
      "    accuracy                           0.77     22741\n",
      "   macro avg       0.70      0.60      0.61     22741\n",
      "weighted avg       0.74      0.77      0.73     22741\n",
      "\n",
      "Testing the model on test data...\n",
      "Test Accuracy Score:  0.6785334121821407\n",
      "Test Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.35      0.44      3055\n",
      "           1       0.70      0.86      0.77      5400\n",
      "\n",
      "    accuracy                           0.68      8455\n",
      "   macro avg       0.65      0.61      0.61      8455\n",
      "weighted avg       0.66      0.68      0.65      8455\n",
      "\n",
      "Training data loaded successfully.\n",
      "Test data loaded successfully.\n",
      "Preprocessing data...\n",
      "Found categorical columns: Index(['sub_grade', 'term', 'home_ownership', 'purpose', 'application_type',\n",
      "       'verification_status'],\n",
      "      dtype='object')\n",
      "Found categorical columns: Index(['sub_grade', 'term', 'home_ownership', 'purpose', 'application_type',\n",
      "       'verification_status'],\n",
      "      dtype='object')\n",
      "Preprocessing complete.\n",
      "Training Random Forest model...\n",
      "Training complete.\n",
      "Testing the model on training data...\n",
      "Training Accuracy Score:  0.764038520733477\n",
      "Training Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.29      0.39      5917\n",
      "           1       0.79      0.93      0.85     16824\n",
      "\n",
      "    accuracy                           0.76     22741\n",
      "   macro avg       0.69      0.61      0.62     22741\n",
      "weighted avg       0.74      0.76      0.73     22741\n",
      "\n",
      "Testing the model on test data...\n",
      "Test Accuracy Score:  0.6756948551153164\n",
      "Test Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.31      0.41      3055\n",
      "           1       0.69      0.88      0.78      5400\n",
      "\n",
      "    accuracy                           0.68      8455\n",
      "   macro avg       0.65      0.60      0.59      8455\n",
      "weighted avg       0.66      0.68      0.64      8455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_df = pd.read_excel('/content/drive/MyDrive/NBFC_Loan_Dataset/train_data.xlsx')\n",
    "    test_df = pd.read_excel('/content/drive/MyDrive/NBFC_Loan_Dataset/test_data.xlsx')\n",
    "\n",
    "    logreg_model = LogisticRegressionModel(train_df, test_df)\n",
    "    rf_model = RandomForestModel(train_df, test_df)\n",
    "\n",
    "\n",
    "    clf_models = {\n",
    "        'Logistic Regression': logreg_model.model,\n",
    "        'Random Forest': rf_model.model\n",
    "    }\n",
    "    # Load data, preprocess, train, test, and predict\n",
    "    logreg_model.load()\n",
    "    logreg_model.preprocess()\n",
    "    logreg_model.train()\n",
    "    logreg_model.test()\n",
    "\n",
    "    # Also testing with the Random Forest model\n",
    "    rf_model = RandomForestModel(train_df, test_df)\n",
    "    rf_model.load()\n",
    "    rf_model.preprocess()\n",
    "    rf_model.train()\n",
    "    rf_model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vTYW-31pm_tD"
   },
   "outputs": [],
   "source": [
    "import pickle # Import the pickle module\n",
    "\n",
    "for name, model in clf_models.items(): # Iterate directly through the items of the dictionary\n",
    "    with open(f\"{name.replace(' ', '_').lower()}_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wINHMb83oU1r"
   },
   "source": [
    "The training pipeline, implemented in the model_.py script, followed a structured, class-based design to build and manage two models: Logistic Regression and Random Forest. The workflow included:\n",
    "\n",
    "Data Loading: Efficient dataset handling was ensured.\n",
    "Preprocessing: Steps such as feature scaling, encoding, and handling missing values were applied.\n",
    "Training: Models were trained and optimized for performance.\n",
    "Testing: Performance was evaluated using a hold-out test set and metrics like accuracy, precision, recall, and F1-score.\n",
    "Prediction: Prediction functions were implemented for accurate inference on new data.\n",
    "Both trained models were serialized as .pkl files for seamless future use without retraining, enabling efficient deployment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
